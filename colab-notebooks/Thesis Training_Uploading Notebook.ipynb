{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Thesis Training/Uploading Notebook.ipynb","provenance":[{"file_id":"1EvnuLDiwQh8yICRIRhX7cQwmzO5ml_lL","timestamp":1642625400226},{"file_id":"14K7bX22Bq3bxFqQwtVXLP0WTohmrjWIR","timestamp":1641951087613},{"file_id":"1xZTXz5WuhWK_glrM6jwx5RM1HgTuzanl","timestamp":1629939540430}],"collapsed_sections":[],"authorship_tag":"ABX9TyME7oHmdhzqVdyoLVrrcc32"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d5538af206f1413b984e74beb2d64647":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_view_name":"VBoxView","_dom_classes":[],"_model_name":"VBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_66fd64a2de064aa087a340d5b2a1664f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_697d9a47efc641a8b557bca2c2ee66f4","IPY_MODEL_dac16302b6a74a098750a3bb6b2eddaa"]}},"66fd64a2de064aa087a340d5b2a1664f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"697d9a47efc641a8b557bca2c2ee66f4":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_view_name":"LabelView","style":"IPY_MODEL_65c4fff3fe07474596233080065b2d69","_dom_classes":[],"description":"","_model_name":"LabelModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0.45MB of 0.45MB uploaded (0.00MB deduped)\r","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b7192da2628b4baab79a8d72f50a65ce"}},"dac16302b6a74a098750a3bb6b2eddaa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e799c3ad46a24b5f9bb765de3309d523","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_742204c7e54f40eba922933ce1d0252d"}},"65c4fff3fe07474596233080065b2d69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b7192da2628b4baab79a8d72f50a65ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e799c3ad46a24b5f9bb765de3309d523":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"742204c7e54f40eba922933ce1d0252d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"640ea01068834c91be82bd754bad887b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e56f985370df4e948ebc481e09d38ae8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2e9e88d1b7cd493aa6b5f7e70eb1fa5f","IPY_MODEL_d52f4e9abb0c48b6812e7803b3508c1b"]}},"e56f985370df4e948ebc481e09d38ae8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2e9e88d1b7cd493aa6b5f7e70eb1fa5f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_48c6375dbf8747e29529a40227b1afc6","_dom_classes":[],"description":"Upload file pytorch_model.bin: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":496311025,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":496311025,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_400b8ff6dbb04997bf1ff8ea356880ee"}},"d52f4e9abb0c48b6812e7803b3508c1b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_76189006373b46d18b92e08f2d36f3fc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 473M/473M [06:00&lt;00:00, 1.38MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_14c6a7b4cb564c759027cc60ea013243"}},"48c6375dbf8747e29529a40227b1afc6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"400b8ff6dbb04997bf1ff8ea356880ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"76189006373b46d18b92e08f2d36f3fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"14c6a7b4cb564c759027cc60ea013243":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"2uUkYGp0PNdQ","executionInfo":{"status":"ok","timestamp":1644005228794,"user_tz":300,"elapsed":27334,"user":{"displayName":"Colin MacRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXjpPrKreBc6gcdj7qfI0hbJGbRB7YEk6MKMhqdg=s64","userId":"13625747244813537090"}}},"source":["%%capture\n","!pip install wandb transformers datasets huggingface-hub \n","!pip install \"tqdm==4.43.0\"\n","!sudo apt-get install git-lfs\n","!git lfs install\n","!git config --global credential.helper store\n","!git config --global user.name \"comacrae\"\n","!git config --global user.email \"comacrae1995@gmail.com\""],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dFaCIlQvP9dY"},"source":["Import wandb and login to WandB, Google Drive, and HF"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"b9X1pgvvPdph","executionInfo":{"status":"ok","timestamp":1644005264633,"user_tz":300,"elapsed":33002,"user":{"displayName":"Colin MacRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXjpPrKreBc6gcdj7qfI0hbJGbRB7YEk6MKMhqdg=s64","userId":"13625747244813537090"}},"outputId":"df9ff43e-05ef-4a6d-f8a1-116ac70899a0"},"source":["import wandb\n","wandb.login()\n","#auth_key = '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'\n","from google.colab import drive\n","drive.mount('/root/content')\n","!huggingface-cli login"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /root/content\n","\n","        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/token.\n","        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n","        \n","Token: \n","Login successful\n","Your token has been saved to /root/.huggingface/token\n"]}]},{"cell_type":"markdown","source":["Read in auth_token for HF and set WANDB_PROJECT environemntal variable"],"metadata":{"id":"Sj7n8FjMFomf"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"M_E3m2pNQBKS","executionInfo":{"status":"ok","timestamp":1644005272111,"user_tz":300,"elapsed":1149,"user":{"displayName":"Colin MacRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXjpPrKreBc6gcdj7qfI0hbJGbRB7YEk6MKMhqdg=s64","userId":"13625747244813537090"}},"outputId":"186f4e46-9b11-42be-e2d8-fc6da58c74aa"},"source":["%env WANDB_PROJECT=thesis_v3\n","with open('/root/.huggingface/token', 'r') as f:\n","  auth_token = f.read()\n","  f.close()\n","auth_token"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["env: WANDB_PROJECT=thesis_v3\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'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'"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"7BgXN66rTjeM"},"source":["Upload FARM QA Dataset and split into training and test datasets "]},{"cell_type":"code","metadata":{"id":"YUyKdhQZTh-h","executionInfo":{"status":"ok","timestamp":1644005275166,"user_tz":300,"elapsed":170,"user":{"displayName":"Colin MacRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXjpPrKreBc6gcdj7qfI0hbJGbRB7YEk6MKMhqdg=s64","userId":"13625747244813537090"}}},"source":["import json\n","import random\n","import numpy as np\n","from pathlib import Path\n","\n","def read_split_squad(path):\n","    path = Path(path)\n","    with open(path, 'rb') as f:\n","        squad_dict = json.load(f)\n","\n","    contexts = {'train': [], 'eval': [], 'val': []}\n","    questions = {'train': [], 'eval': [], 'val': []}\n","    answers = {'train': [], 'eval': [], 'val': []}\n","    for group in squad_dict['data']:\n","        for passage in group['paragraphs']:\n","            context = passage['context']\n","            for qa in passage['qas']:\n","                question = qa['question']\n","                split = qa['split']\n","                for answer in qa['answers']:\n","                  contexts[split].append(context)\n","                  questions[split].append(question)\n","                  answers[split].append(answer)\n","\n","    return contexts, questions, answers\n","\n","def add_end_idx(answers, contexts):\n","  \n","    for answer, context in zip(answers,contexts):\n","        gold_text = answer['text']\n","        start_idx = answer['answer_start']\n","        end_idx = start_idx + len(gold_text)\n","\n","        # sometimes squad answers are off by a character or two – fix this\n","        if context[start_idx:end_idx] == gold_text:\n","            answer['answer_end'] = end_idx\n","        elif context[start_idx-1:end_idx-1] == gold_text:\n","            answer['answer_start'] = start_idx - 1\n","            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n","        elif context[start_idx-2:end_idx-2] == gold_text:\n","            answer['answer_start'] = start_idx - 2\n","            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n","\n","def add_token_positions(encodings, answers):\n","    start_positions = []\n","    end_positions = []\n","    for i in range(len(answers)):\n","        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n","        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n","\n","        # if start position is None, the answer passage has been truncated\n","        if start_positions[-1] is None:\n","            start_positions[-1] = tokenizer.model_max_length\n","        if end_positions[-1] is None:\n","            end_positions[-1] = tokenizer.model_max_length\n","\n","    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n","\n","def encode_split_data(tokenizer, contexts, questions, answers):\n","  splits = ['train', 'val', 'eval']\n","  encoding_dict = dict.fromkeys(splits)\n","  for split in splits:\n","    encodings = tokenizer(contexts[split], questions[split], truncation=True, padding=True)\n","    add_token_positions(encodings, answers[split])\n","    encoding_dict[split] = encodings\n","  \n","  return encoding_dict\n","\n","      \n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ITzGXEhPjjEK"},"source":["Read in  pre-split data"]},{"cell_type":"code","metadata":{"id":"lDone8VZjl34","executionInfo":{"status":"ok","timestamp":1644009092410,"user_tz":300,"elapsed":1671,"user":{"displayName":"Colin MacRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXjpPrKreBc6gcdj7qfI0hbJGbRB7YEk6MKMhqdg=s64","userId":"13625747244813537090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"af5acc0e-5cdc-4e50-a581-03c30f291d6b"},"source":["import random\n","contexts, questions, answers = read_split_squad('/root/content/My Drive/hitm/training-datasets/newedapara.json')\n","for split in ['train', 'eval', 'val']:\n","  add_end_idx(answers[split], contexts[split])\n","\n","train_size = len(contexts['train'])\n","eval_size = len(contexts['eval'])\n","val_size = len(contexts['val'])\n","total = train_size + eval_size + val_size\n","print(f'Total: {total} | Train: {train_size} | Eval: {eval_size} | Val: {val_size}')\n"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Total: 22318 | Train: 21864 | Eval: 227 | Val: 227\n"]}]},{"cell_type":"markdown","metadata":{"id":"Oces9DmgD5a5"},"source":["Set up tokenizer and encode splits"]},{"cell_type":"code","metadata":{"id":"F7mlrnXsD8Ds","executionInfo":{"status":"ok","timestamp":1644009135041,"user_tz":300,"elapsed":39650,"user":{"displayName":"Colin MacRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXjpPrKreBc6gcdj7qfI0hbJGbRB7YEk6MKMhqdg=s64","userId":"13625747244813537090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"388a352f-6fe2-4f14-b965-6749fd361ff5"},"source":["from transformers import AutoTokenizer\n","model_name = 'deepset/roberta-base-squad2'\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n","\n","encoding_dict = encode_split_data(tokenizer, contexts, questions, answers)"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/deepset/roberta-base-squad2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c40d0abb589629c48763f271020d0b1f602f5208c432c0874d420491ed37e28b.122ed338b3591c07dba452777c59ff52330edb340d3d56d67aa9117ad9905673\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"deepset/roberta-base-squad2\",\n","  \"architectures\": [\n","    \"RobertaForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"language\": \"english\",\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"name\": \"Roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading file https://huggingface.co/deepset/roberta-base-squad2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/81c80edb4c6cefa5cae64ccfdb34b3b309ecaf60da99da7cd1c17e24a5d36eb5.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","loading file https://huggingface.co/deepset/roberta-base-squad2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/b87d46371731376b11768b7839b1a5938a4f77d6bd2d9b683f167df0026af432.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/deepset/roberta-base-squad2/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/deepset/roberta-base-squad2/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/deepset/roberta-base-squad2/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/c9d2c178fac8d40234baa1833a3b1903d393729bf93ea34da247c07db24900d0.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n","loading file https://huggingface.co/deepset/roberta-base-squad2/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/e8a600814b69e3ee74bb4a7398cc6fef9812475010f16a6c9f151b2c2772b089.451739a2f3b82c3375da0dfc6af295bedc4567373b171f514dd09a4cc4b31513\n","loading configuration file https://huggingface.co/deepset/roberta-base-squad2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c40d0abb589629c48763f271020d0b1f602f5208c432c0874d420491ed37e28b.122ed338b3591c07dba452777c59ff52330edb340d3d56d67aa9117ad9905673\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"deepset/roberta-base-squad2\",\n","  \"architectures\": [\n","    \"RobertaForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"language\": \"english\",\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"name\": \"Roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file https://huggingface.co/deepset/roberta-base-squad2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c40d0abb589629c48763f271020d0b1f602f5208c432c0874d420491ed37e28b.122ed338b3591c07dba452777c59ff52330edb340d3d56d67aa9117ad9905673\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"deepset/roberta-base-squad2\",\n","  \"architectures\": [\n","    \"RobertaForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"language\": \"english\",\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"name\": \"Roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"e99u_3IURpPA"},"source":["Create dataset\n","\n"]},{"cell_type":"code","metadata":{"id":"18hdsqf0RsJ1","executionInfo":{"status":"ok","timestamp":1644009141884,"user_tz":300,"elapsed":186,"user":{"displayName":"Colin MacRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXjpPrKreBc6gcdj7qfI0hbJGbRB7YEk6MKMhqdg=s64","userId":"13625747244813537090"}}},"source":["import torch\n","\n","class SquadDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings):\n","        self.encodings = encodings\n","\n","    def __getitem__(self, idx):\n","        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","\n","    def __len__(self):\n","        return len(self.encodings.input_ids)\n","\n","\n","train_dataset = SquadDataset(encoding_dict['train'])\n","eval_dataset = SquadDataset(encoding_dict['eval'])\n","if(len(encoding_dict['val']) > 0):\n","  validation_dataset = SquadDataset(encoding_dict['val'])\n","\n","\n"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2By6JjIDQQlp"},"source":["Set up HF Trainer"]},{"cell_type":"code","metadata":{"id":"5K5KNIJOQPsV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644009152820,"user_tz":300,"elapsed":8340,"user":{"displayName":"Colin MacRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXjpPrKreBc6gcdj7qfI0hbJGbRB7YEk6MKMhqdg=s64","userId":"13625747244813537090"}},"outputId":"472ce868-e5e7-4b1a-c3fa-24f2005b0209"},"source":["from transformers import TrainingArguments, Trainer, AutoModelForQuestionAnswering\n","model = AutoModelForQuestionAnswering.from_pretrained(pretrained_model_name_or_path=model_name)\n","run_name = input(\"enter run name: \") \n","output_dir = f\"/root/content/My Drive/hitm/models/{model_name}-{run_name}\"\n","\n","args = TrainingArguments(\n","    logging_steps=10,\n","    do_train=True,\n","    do_eval=True,\n","    report_to=\"wandb\",\n","    run_name=run_name,\n","    evaluation_strategy='steps', \n","    #num_train_epochs=1, \n","    max_steps=500,\n","    eval_steps = 10,\n","    output_dir=output_dir,\n","    overwrite_output_dir =True\n",")\n","\n","trainer=Trainer(model=model, args=args, train_dataset=train_dataset, eval_dataset=validation_dataset)\n"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/deepset/roberta-base-squad2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c40d0abb589629c48763f271020d0b1f602f5208c432c0874d420491ed37e28b.122ed338b3591c07dba452777c59ff52330edb340d3d56d67aa9117ad9905673\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"deepset/roberta-base-squad2\",\n","  \"architectures\": [\n","    \"RobertaForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"language\": \"english\",\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"name\": \"Roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/deepset/roberta-base-squad2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/eac3273a8097dda671e3bea1db32c616e74f36a306c65b4858171c98d6db83e9.084aa7284f3a51fa1c8f0641aa04c47d366fbd18711f29d0a995693cfdbc9c9e\n","All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n","\n","All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at deepset/roberta-base-squad2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["enter run name: edapara\n"]},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","max_steps is given, it will override any value given in num_train_epochs\n"]}]},{"cell_type":"markdown","metadata":{"id":"w62KkCWzJitr"},"source":["1And now we train! Our wandb page will be populated with training information."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d5538af206f1413b984e74beb2d64647","66fd64a2de064aa087a340d5b2a1664f","697d9a47efc641a8b557bca2c2ee66f4","dac16302b6a74a098750a3bb6b2eddaa","65c4fff3fe07474596233080065b2d69","b7192da2628b4baab79a8d72f50a65ce","e799c3ad46a24b5f9bb765de3309d523","742204c7e54f40eba922933ce1d0252d"]},"id":"zCLu6-0_Ishq","executionInfo":{"status":"ok","timestamp":1644009624903,"user_tz":300,"elapsed":465977,"user":{"displayName":"Colin MacRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXjpPrKreBc6gcdj7qfI0hbJGbRB7YEk6MKMhqdg=s64","userId":"13625747244813537090"}},"outputId":"f8bd4a80-e223-4049-be40-7f584b17bce0"},"source":["trainer.train()\n","wandb.finish()"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 21864\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 500\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/comacrae95/thesis_v3/runs/184wwu77\" target=\"_blank\">edapara</a></strong> to <a href=\"https://wandb.ai/comacrae95/thesis_v3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 07:33, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>3.623800</td>\n","      <td>2.254718</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>2.441400</td>\n","      <td>2.309167</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>2.474000</td>\n","      <td>1.964217</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>2.339600</td>\n","      <td>2.033496</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>2.382200</td>\n","      <td>1.867031</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.953000</td>\n","      <td>1.883337</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>1.768900</td>\n","      <td>1.694973</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>2.066100</td>\n","      <td>1.805569</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>1.673900</td>\n","      <td>1.735792</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.814300</td>\n","      <td>1.897399</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>1.477300</td>\n","      <td>1.766898</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>1.525500</td>\n","      <td>1.917261</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>2.123700</td>\n","      <td>1.749890</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>1.587500</td>\n","      <td>1.807709</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.612700</td>\n","      <td>1.681687</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>1.517400</td>\n","      <td>1.666082</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>1.686700</td>\n","      <td>1.770875</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>1.509500</td>\n","      <td>2.026638</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>1.431700</td>\n","      <td>1.796649</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.537800</td>\n","      <td>1.579460</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>1.297200</td>\n","      <td>1.647864</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>1.567400</td>\n","      <td>1.703552</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>1.415100</td>\n","      <td>1.673760</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>1.508600</td>\n","      <td>1.638814</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>1.414000</td>\n","      <td>1.617420</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>1.265500</td>\n","      <td>1.574412</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>1.120600</td>\n","      <td>1.832771</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>1.402700</td>\n","      <td>1.703322</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.995400</td>\n","      <td>1.645834</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.228300</td>\n","      <td>1.648419</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.946500</td>\n","      <td>1.667540</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>1.291000</td>\n","      <td>1.755204</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>1.708200</td>\n","      <td>1.695083</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>1.324900</td>\n","      <td>1.819986</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>1.295500</td>\n","      <td>1.749136</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>1.072200</td>\n","      <td>1.646966</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.852600</td>\n","      <td>1.727695</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>1.134700</td>\n","      <td>1.783957</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.860800</td>\n","      <td>1.819193</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.268000</td>\n","      <td>1.679261</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>1.809900</td>\n","      <td>1.555071</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>1.131700</td>\n","      <td>1.533532</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>1.175100</td>\n","      <td>1.610439</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>0.777400</td>\n","      <td>1.627197</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>1.008600</td>\n","      <td>1.656276</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>0.904300</td>\n","      <td>1.679880</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>1.055500</td>\n","      <td>1.672307</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>1.211200</td>\n","      <td>1.658273</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>0.822700</td>\n","      <td>1.659502</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.109500</td>\n","      <td>1.663360</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 227\n","  Batch size = 8\n","Saving model checkpoint to /root/content/My Drive/hitm/models/deepset/roberta-base-squad2-edapara/checkpoint-500\n","Configuration saved in /root/content/My Drive/hitm/models/deepset/roberta-base-squad2-edapara/checkpoint-500/config.json\n","Model weights saved in /root/content/My Drive/hitm/models/deepset/roberta-base-squad2-edapara/checkpoint-500/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"display_data","data":{"text/html":["<br/>Waiting for W&B process to finish, PID 1615... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5538af206f1413b984e74beb2d64647","version_minor":0,"version_major":2},"text/plain":["VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\">\n","<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▅▆▄▂▃▃▃▄▃▃▂▃▅▃▂▃▂▂▁▄▃▂▂▃▂▄▂▃▃▄▁▁▂▂▂▂▂▂</td></tr><tr><td>eval/runtime</td><td>▅▅▆▃▄▂▃▄█▁▂▆▆▆▇██▂▂▅▄▅▄▂▃▅▅▄▆▅▄▂▅▅███▄▁▇</td></tr><tr><td>eval/samples_per_second</td><td>▃▃▃▆▅▇▆▅▁█▇▃▃▃▂▁▁▇▇▄▅▄▅▇▆▄▄▅▃▄▅▇▄▄▁▁▁▅█▂</td></tr><tr><td>eval/steps_per_second</td><td>▄▃▃▆▅▇▆▅▁█▇▃▃▃▂▁▁▇▇▅▅▅▅▇▆▄▄▅▃▄▅▇▄▄▁▁▁▅█▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▅▅▄▃▄▃▃▃▄▃▃▃▃▃▂▃▃▃▂▂▃▂▁▂▃▂▂▁▂▁▄▂▂▁▁▂▂▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n","<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.66336</td></tr><tr><td>eval/runtime</td><td>4.271</td></tr><tr><td>eval/samples_per_second</td><td>53.149</td></tr><tr><td>eval/steps_per_second</td><td>6.79</td></tr><tr><td>train/epoch</td><td>0.18</td></tr><tr><td>train/global_step</td><td>500</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1095</td></tr><tr><td>train/total_flos</td><td>1045187026944000.0</td></tr><tr><td>train/train_loss</td><td>1.49044</td></tr><tr><td>train/train_runtime</td><td>460.0727</td></tr><tr><td>train/train_samples_per_second</td><td>8.694</td></tr><tr><td>train/train_steps_per_second</td><td>1.087</td></tr></table>\n","</div></div>\n","Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","<br/>Synced <strong style=\"color:#cdcd00\">edapara</strong>: <a href=\"https://wandb.ai/comacrae95/thesis_v3/runs/184wwu77\" target=\"_blank\">https://wandb.ai/comacrae95/thesis_v3/runs/184wwu77</a><br/>\n","Find logs at: <code>./wandb/run-20220204_211238-184wwu77/logs</code><br/>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"markdown","source":["save tokenizer and finetuned model"],"metadata":{"id":"RZS5YaUSOIU_"}},{"cell_type":"code","source":["dir_name = 'roberta-edaparav2' # change as desired!!!\n","tokenizer_dir_path=f'/root/content/My Drive/hitm/tokenizers/{dir_name}'\n","tokenizer.save_pretrained(tokenizer_dir_path)\n","model_dir_path =f\"/root/content/My Drive/hitm/models/{dir_name}\"\n","model.save_pretrained(model_dir_path)\n"],"metadata":{"id":"LNSJVXxCRVu5","executionInfo":{"status":"ok","timestamp":1644009645796,"user_tz":300,"elapsed":3244,"user":{"displayName":"Colin MacRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXjpPrKreBc6gcdj7qfI0hbJGbRB7YEk6MKMhqdg=s64","userId":"13625747244813537090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"198325fe-42d3-4293-f30e-9e7c0c8332df"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["tokenizer config file saved in /root/content/My Drive/hitm/tokenizers/roberta-edaparav2/tokenizer_config.json\n","Special tokens file saved in /root/content/My Drive/hitm/tokenizers/roberta-edaparav2/special_tokens_map.json\n","Configuration saved in /root/content/My Drive/hitm/models/roberta-edaparav2/config.json\n","Model weights saved in /root/content/My Drive/hitm/models/roberta-edaparav2/pytorch_model.bin\n"]}]},{"cell_type":"markdown","source":["Create HF repo, clone, and upload files\n","\n","Note:push_to_hub is finnicky. If repo_path_or_name is a param,\n","cd to the repo and use \"./\". If it's just repo_name, use repo_url=\"\\<insert url here\\>\""],"metadata":{"id":"MJU5S1BmUDS_"}},{"cell_type":"code","source":["#create repo in HF !huggingface-cli repo create <MODEL-NAME>\n","#clone newly created repo: !git clone https://huggingface.co/<HF USERNAME>/<MODEL-NAME>\n","!huggingface-cli repo create $dir_name\n","!git clone https://huggingface.co/comacrae/$dir_name\n","%cd ./$dir_name/\n","tokenizer.push_to_hub(repo_path_or_name=\"./\",use_auth_token=True) #\n","model.push_to_hub(repo_path_or_name=\"./\",use_auth_token=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":640,"referenced_widgets":["640ea01068834c91be82bd754bad887b","e56f985370df4e948ebc481e09d38ae8","2e9e88d1b7cd493aa6b5f7e70eb1fa5f","d52f4e9abb0c48b6812e7803b3508c1b","48c6375dbf8747e29529a40227b1afc6","400b8ff6dbb04997bf1ff8ea356880ee","76189006373b46d18b92e08f2d36f3fc","14c6a7b4cb564c759027cc60ea013243"]},"id":"YYnzK1wWONV3","executionInfo":{"status":"ok","timestamp":1644010044464,"user_tz":300,"elapsed":395371,"user":{"displayName":"Colin MacRae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXjpPrKreBc6gcdj7qfI0hbJGbRB7YEk6MKMhqdg=s64","userId":"13625747244813537090"}},"outputId":"194d7f45-b27d-4906-f9ac-56c51aedfa1d"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[90mgit version 2.17.1\u001b[0m\n","Error: unknown flag: --version\n","\n","\u001b[90mSorry, no usage text found for \"git-lfs\"\u001b[0m\n","\n","You are about to create \u001b[1mcomacrae/roberta-edaparav2\u001b[0m\n","Proceed? [Y/n] y\n","\n","Your repo now lives at:\n","  \u001b[1mhttps://huggingface.co/comacrae/roberta-edaparav2\u001b[0m\n","\n","You can clone it locally with the command below, and commit/push as usual.\n","\n","  git clone https://huggingface.co/comacrae/roberta-edaparav2\n","\n","Cloning into 'roberta-edaparav2'...\n","remote: Enumerating objects: 3, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (2/2), done.\u001b[K\n","remote: Total 3 (delta 0), reused 0 (delta 0)\u001b[K\n","Unpacking objects: 100% (3/3), done.\n","/content/roberta-unaugv2/roberta-edav2/roberta-edav2.0/roberta-edaparav2\n"]},{"output_type":"stream","name":"stderr","text":["tokenizer config file saved in ./tokenizer_config.json\n","Special tokens file saved in ./special_tokens_map.json\n","To https://huggingface.co/comacrae/roberta-edaparav2\n","   5573583..7487602  main -> main\n","\n","Configuration saved in ./config.json\n","Model weights saved in ./pytorch_model.bin\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"640ea01068834c91be82bd754bad887b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Upload file pytorch_model.bin', max=496311025.0, style=Pr…"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["To https://huggingface.co/comacrae/roberta-edaparav2\n","   7487602..d6156fc  main -> main\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'https://huggingface.co/comacrae/roberta-edaparav2/commit/d6156fc7a2e9ce1d59f59989ee517bad8594005e'"]},"metadata":{},"execution_count":34}]}]}